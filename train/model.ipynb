{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90f4a40d-dd8d-4f3b-88c9-8ca823d9bf50",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612bcd96-3627-4f54-92f5-482135034a6e",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb4d3a1-0630-4915-aaa2-79d33b40a23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from gensim.models import KeyedVectors\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddbd382-8895-4cc0-a69c-ba253820cdf2",
   "metadata": {},
   "source": [
    "## Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162e08c8-c07b-4027-8bd5-412dc21196bd",
   "metadata": {},
   "source": [
    "- \"train_x\", \"test_x\", \"valid_x\"\n",
    "- \"train_y\", \"test_y\", \"valid_y\"\n",
    "- \"train_vocab\", \"test_vocab\", \"valid_vocab\"\n",
    "- \"train_sub\", \"test_sub\", \"valid_sub\"\n",
    "- \"token_to_idx\", \"label_to_idx\", \"domain_to_idx\", \"hash_to_terminal\"\n",
    "- \"domain_classes\"\n",
    "- \"embeddings\", \"model_path\"\n",
    "- \"kernel_size\", \"num_filters\", \"embedding_dim\"\n",
    "- \"dropout\", \"weight_decay\", \"learning_rate\", \"batch_size\", \"num_epochs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd396aa-176e-4067-9489-fac2f9a0bbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"../model_preferences.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4404e1-5fad-4859-9b31-4b561d4fabc9",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121eea2a-d8d4-428a-a8d1-2a2ffd6af786",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnDataset:\n",
    "    def __init__(self, tokens_file, labels_file, token_to_idx_file, label_to_idx_file):\n",
    "        self.tokens_list = []\n",
    "        self.labels = []\n",
    "        self.token_to_idx = {}\n",
    "        self.label_to_idx = {}\n",
    "\n",
    "        with open(tokens_file, \"r\", encoding=\"UTF-8\") as f:\n",
    "            for x in tqdm(f):\n",
    "                x = x.replace('\\x00', '').strip()\n",
    "                self.tokens_list.append(x.split(u' '))\n",
    "        with open(labels_file, \"r\", encoding=\"UTF-8\") as f:\n",
    "            for x in tqdm(f):\n",
    "                x = x.replace('\\x00', '').strip()\n",
    "                self.labels.append(x.split('\\n')[0])\n",
    "        self.token_to_idx[\"@@PAD@@\"] = 0\n",
    "        self.token_to_idx[\"@@UNK@@\"] = 1\n",
    "        idx = 2\n",
    "        with open(token_to_idx_file, \"r\", encoding=\"UTF-8\") as f:\n",
    "            for x in tqdm(f):\n",
    "                x = x.replace('\\x00', '').strip()\n",
    "                self.token_to_idx[x.split('\\n')[0]] = idx\n",
    "                idx += 1\n",
    "        idx = 0\n",
    "        with open(label_to_idx_file, \"r\", encoding=\"UTF-8\") as f:\n",
    "            for x in tqdm(f):\n",
    "                x = x.replace('\\x00', '').strip()\n",
    "                self.label_to_idx[x.split('\\n')[0]] = idx\n",
    "                idx += 1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tokens_list)\n",
    "\n",
    "    def numericalize(self, tokens):\n",
    "        indices = [self.token_to_idx.get(token) for token in tokens]\n",
    "        \n",
    "        return np.array(indices, dtype=np.int64)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokens_list[idx]\n",
    "        label = self.labels[idx]\n",
    "        label = self.label_to_idx.get(label)\n",
    "        indices = self.numericalize(tokens)\n",
    "        return torch.tensor(indices), torch.tensor(label, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfaecf4-963a-4578-9902-6be73242a1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnDataloader:\n",
    "    def __init__(self, dataset, batch_size=1024, shuffle=True):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(dataset))\n",
    "        \n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "        \n",
    "        for start_idx in range(0, len(self.dataset), self.batch_size):\n",
    "            batch_indices = self.indices[start_idx:start_idx + self.batch_size]\n",
    "            batch_tokens, batch_labels = [], []\n",
    "            for idx in batch_indices:\n",
    "                token, label = self.dataset[idx]\n",
    "                batch_tokens.append(token)\n",
    "                batch_labels.append(label)\n",
    "            \n",
    "            batch_tokens = pad_sequence(batch_tokens, batch_first=True, padding_value=0)\n",
    "            batch_labels = torch.stack(batch_labels)\n",
    "            yield batch_tokens, batch_labels\n",
    "            \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.dataset) / self.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d6fd7a-bee0-4716-936f-fc97b0c16db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_path) as fp:\n",
    "    config = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2910f8-e5d2-40a3-90b0-87d968af0cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CnnDataset(config[\"train_x\"], config[\"train_y\"], config[\"token_to_idx\"], config[\"label_to_idx\"])\n",
    "valid_dataset = CnnDataset(config[\"valid_x\"], config[\"valid_y\"], config[\"token_to_idx\"], config[\"label_to_idx\"])\n",
    "test_dataset = CnnDataset(config[\"test_x\"], config[\"test_y\"], config[\"token_to_idx\"], config[\"label_to_idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216e803a-af00-41c9-88a3-aae221b60280",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = CnnDataloader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "val_loader   = CnnDataloader(valid_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "test_loader  = CnnDataloader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00602d4b-50e9-4128-973c-2f4331ec5538",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1f32e7-f9e4-4152-8c6f-01432d6a70ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"embeddings\"] !=  \"\":\n",
    "    w2v = KeyedVectors.load_word2vec_format(config[\"embeddings\"], binary=True)\n",
    "    vocab_size = len(train_dataset.token_to_idx)\n",
    "    embeddings = np.zeros((vocab_size, config[\"embedding_dim\"]))\n",
    "    embeddings[train_dataset.token_to_idx['@@UNK@@']] = normalize(np.random.uniform(-0.25, 0.25, (1, config[\"embedding_dim\"])), norm='l2', axis=1)\n",
    "    embeddings[train_dataset.token_to_idx['@@PAD@@']] = np.zeros((1, config[\"embedding_dim\"]))\n",
    "    for word, idx in train_dataset.token_to_idx.items():\n",
    "        if word in w2v.wv:  \n",
    "            embeddings[idx] = w2v.wv[word]\n",
    "    embeddings = torch.tensor(embeddings, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aac5ba5-7eb9-4a9b-add0-e9b4636dfab6",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe84083-d754-4842-ba3d-b46bce007b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeCNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 kernel_size, \n",
    "                 embedding_dim, \n",
    "                 num_filters, \n",
    "                 padding_idx, \n",
    "                 num_labels,\n",
    "                 num_classes, \n",
    "                 vocab_size,\n",
    "                 dropout=0.5,\n",
    "                 pretrained_embeddings=None,\n",
    "                 pretrained_domains=None,\n",
    "                 freeze_embedding=False,\n",
    "                 freeze_domains=False):\n",
    "        super(CodeCNN, self).__init__()\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_filters = num_filters\n",
    "        self.padding_idx = padding_idx\n",
    "        self.num_labels = num_labels\n",
    "        self.num_classes = num_classes\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.num_domains = num_labels // num_classes\n",
    "\n",
    "        # Pads the input tensor boundaries with a constant value.\n",
    "        # padding (int, tuple) = (kernel_size - 1, 0)\n",
    "        self.pad = nn.ConstantPad1d(self.kernel_size - 1, self.padding_idx)\n",
    "\n",
    "        # Embedding layer\n",
    "        # [emb_dim, vocab_size]\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, \n",
    "                                                          freeze=freeze_embedding)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(num_embeddings=self.vocab_size,\n",
    "                                          embedding_dim=self.embedding_dim,\n",
    "                                          padding_idx=self.padding_idx,\n",
    "                                          max_norm=1.0)\n",
    "\n",
    "        # 1-D Convolution\n",
    "        # [emb_dim * kernel_size, num_filters]\n",
    "        self.conv = nn.Conv1d(1, self.num_filters, self.embedding_dim * self.kernel_size,\n",
    "                              stride=self.embedding_dim, bias=True)\n",
    "        \n",
    "        # Matrix that represents domains \n",
    "        # [num_filters, num_domains]\n",
    "        if pretrained_domains is not None:\n",
    "            pretrained_domains = F.normalize(pretrained_domains, p=2, dim=1)\n",
    "            self.attention_domains = nn.Embedding.from_pretrained(pretrained_domains,\n",
    "                                                                  freeze=freeze_domains)\n",
    "        else:\n",
    "            self.attention_domains = nn.Embedding(num_embeddings=self.num_domains,\n",
    "                                                  embedding_dim=self.num_filters,\n",
    "                                                  padding_idx=self.padding_idx,\n",
    "                                                  max_norm=1.0)\n",
    "            nn.init.xavier_uniform_(self.attention_domains.weight)\n",
    "\n",
    "        # List of fc-layers: one for each domain\n",
    "        # [num_filters, num_classes]\n",
    "        self.fcs = nn.ModuleList([\n",
    "            nn.Linear(self.num_filters, self.num_classes)\n",
    "            for _ in range(self.num_domains)\n",
    "        ])\n",
    "\n",
    "        # Batch normalization\n",
    "        # [num_filters]\n",
    "        self.bn_conv = nn.BatchNorm1d(self.num_filters)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        data, domain = inputs\n",
    "        \n",
    "        # Make all sentences equal in length\n",
    "        # [batch_size, seq_len]\n",
    "        x = self.pad(data)\n",
    "        \n",
    "        # Apply embedding layer to each word\n",
    "        # [batch, seq_len, emb_dim]\n",
    "        embedded_x = self.embedding(x).float() \n",
    "        \n",
    "        # Reshape \n",
    "        # [batch_size, 1, seq_len * embedding_dim]\n",
    "        embedded_x = embedded_x.view(-1, 1, x.shape[1] * self.embedding_dim)\n",
    "        # Apply all convolution filters in parallel\n",
    "        # [batch_size, num_filters, seq_len]\n",
    "        features = self.conv(embedded_x)  \n",
    "\n",
    "        activations = self.bn_conv(features)\n",
    "\n",
    "        # ReLU\n",
    "        activations = F.relu(activations)\n",
    " \n",
    "        # Dropout\n",
    "        activations = self.dropout(activations)\n",
    "        \n",
    "        # Choose domains\n",
    "        # [batch_size, num_filters]\n",
    "        domains = self.attention_domains(domain)\n",
    "\n",
    "        # Get attention weights\n",
    "        # [batch_size, seq_len]\n",
    "        attention_weights = torch.einsum('bf,bfs->bs', domains, activations)\n",
    "        attention_weights = F.softmax(attention_weights, dim=1)\n",
    "\n",
    "        # Compute the weighted sum\n",
    "        # [batch_size, num_filters]\n",
    "        result = torch.einsum('bs,bfs->bf', attention_weights, activations)\n",
    "        \n",
    "        # Get logits\n",
    "        # [batch_size, num_domains]\n",
    "        logits = torch.stack([\n",
    "            self.fcs[domain[i]](result[i])\n",
    "            for i in range(result.shape[0])\n",
    "        ])\n",
    "        \n",
    "        return {'features': features,\n",
    "                'activations': activations,\n",
    "                'domains': domains,\n",
    "                'logits': logits}\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        return self.embedding.weight\n",
    "        \n",
    "    def get_filters(self):\n",
    "        return self.conv.weight.squeeze(), self.conv.bias\n",
    "\n",
    "    def get_attention_domains(self):\n",
    "        return self.attention_domains.weight\n",
    "    \n",
    "    def get_fc_weights(self):\n",
    "        return [(fc.weight.data, fc.bias.data) for fc in self.fcs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9460c6a-7c97-48e6-a8b6-92f96013d2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, valid_loader, test_loader, config):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if config.get(\"cuda\", False) else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=config.get(\"learning_rate\", 1e-3), weight_decay=config.get(\"weight_decay\", 1e-4))\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def _run_epoch(self, loader, desc=\"Evaluating\"):\n",
    "        epoch_loss = 0\n",
    "        corrects = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        num_classes = len(self.config[\"domain_classes\"])\n",
    "\n",
    "        for batch in tqdm(loader, desc=desc):\n",
    "            batch_x, batch_y = batch \n",
    "            batch_x = torch.LongTensor(batch_x).to(self.device)\n",
    "            batch_y = torch.LongTensor(batch_y).to(self.device)\n",
    "            \n",
    "            domain = batch_y // num_classes  \n",
    "            \n",
    "            outputs = self.model((batch_x, domain))\n",
    "            logits = outputs['logits']\n",
    "            loss = self.criterion(logits, batch_y % num_classes)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            pred_labels = torch.max(logits, 1)[1]\n",
    "            corrects += (pred_labels == (batch_y % num_classes)).sum().item()\n",
    "            \n",
    "            all_preds.extend(pred_labels.cpu().numpy())\n",
    "            all_labels.extend((batch_y % num_classes).cpu().numpy())\n",
    "\n",
    "        avg_loss = epoch_loss / len(loader)\n",
    "        accuracy = corrects / (len(loader.dataset)) * 100\n",
    "        f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "        precision = precision_score(all_labels, all_preds, average=\"macro\")\n",
    "        recall = recall_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "        return avg_loss, accuracy, f1, precision, recall\n",
    "\n",
    "    def train_epoch(self):\n",
    "        \"\"\"\n",
    "        Run one training epoch.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        epoch_loss = 0\n",
    "        corrects = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        num_classes = len(self.config[\"domain_classes\"])\n",
    "\n",
    "        for batch in tqdm(self.train_loader, desc=\"Training\"):\n",
    "            batch_x, batch_y = batch  # adjust based on your DataLoader\n",
    "            batch_x = torch.LongTensor(batch_x).to(self.device)\n",
    "            batch_y = torch.LongTensor(batch_y).to(self.device)\n",
    "            domain = batch_y // num_classes\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model((batch_x, domain))\n",
    "            logits = outputs['logits']\n",
    "            loss = self.criterion(logits, batch_y % num_classes)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            pred_labels = torch.max(logits, 1)[1]\n",
    "            corrects += (pred_labels == (batch_y % num_classes)).sum().item()\n",
    "            all_preds.extend(pred_labels.cpu().numpy())\n",
    "            all_labels.extend((batch_y % num_classes).cpu().numpy())\n",
    "\n",
    "        avg_loss = epoch_loss / len(self.train_loader)\n",
    "        accuracy = corrects / (len(self.train_loader.dataset)) * 100\n",
    "        f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "        precision = precision_score(all_labels, all_preds, average=\"macro\")\n",
    "        recall = recall_score(all_labels, all_preds, average=\"macro\")\n",
    "        \n",
    "        return avg_loss, accuracy, f1, precision, recall\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            return self._run_epoch(self.valid_loader, desc=\"Validating\")\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            return self._run_epoch(self.test_loader, desc=\"Testing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ef029d-588e-4aa9-9299-30c88e00c994",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa66142-eb87-4705-88f1-01cda34babda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "model_name = \"model\"\n",
    "torch.cuda.set_device(0)\n",
    "#######################\n",
    "concrete_model_path = config[\"model_path\"] + \"/\" + model_name;\n",
    "log_path = concrete_model_path + \"/logs\"\n",
    "weights_path = concrete_model_path + \"/weights\"\n",
    "\n",
    "if not os.path.exists(config[\"model_path\"]):\n",
    "    os.makedirs(config[\"model_path\"])\n",
    "if not os.path.exists(concrete_model_path):\n",
    "    os.makedirs(concrete_model_path)\n",
    "if not os.path.exists(log_path):\n",
    "    os.makedirs(log_path)\n",
    "if not os.path.exists(weights_path):\n",
    "    os.makedirs(weights_path)\n",
    "\n",
    "model = CodeCNN(kernel_size=config[\"kernel_size\"], embedding_dim=config[\"embedding_dim\"],\n",
    "                            num_filters=config[\"num_filters\"], padding_idx=0,\n",
    "                            num_labels=len(train_dataset.label_to_idx), num_classes=len(config[\"domain_classes\"]), vocab_size=len(train_dataset.token_to_idx))\n",
    "trainer = Trainer(model, train_loader, val_loader, test_loader, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8746ee5-ee08-4447-a1d6-1f971ca8b624",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\t\".join([\"Epoch\", \"Train Loss\", \"Train Acc\", \"Train F1\", \"Train Precision\", \"Train Recall\", \"Eval Loss\", \"Eval Acc\", \"Eval F1\", \"Eval Precision\", \"Evall Recall\", \"Best Eval Acc\"]))\n",
    "metrics = {\"train_loss\": [], \"train_acc\": [], \"train_f1\":[], \"train_prec\":[], \"train_rec\":[], \"eval_loss\": [], \"eval_acc\": [], \"eval_f1\":[], \"eval_prec\":[], \"eval_rec\":[], \"best\": -1}\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "for epoch in range(config[\"num_epochs\"]):\n",
    "    train_loss, train_acc, train_f1, train_prec, train_rec = trainer.train_epoch()\n",
    "    eval_loss, eval_acc, eval_f1, eval_prec, eval_rec = trainer.validate()\n",
    "    metrics[\"train_loss\"].append(train_loss)\n",
    "    metrics[\"train_acc\"].append(train_acc)\n",
    "    metrics[\"train_f1\"].append(train_f1)\n",
    "    metrics[\"train_prec\"].append(train_prec)\n",
    "    metrics[\"train_rec\"].append(train_rec)\n",
    "    \n",
    "    metrics[\"eval_loss\"].append(eval_loss)\n",
    "    metrics[\"eval_acc\"].append(eval_acc)\n",
    "    metrics[\"eval_f1\"].append(eval_f1)\n",
    "    metrics[\"eval_prec\"].append(eval_prec)\n",
    "    metrics[\"eval_rec\"].append(eval_rec)\n",
    "\n",
    "    if eval_acc > metrics[\"best\"]:\n",
    "        metrics[\"best\"] = eval_acc\n",
    "        torch.save(model, concrete_model_path + \"/model.pt\")\n",
    "        \n",
    "    if eval_loss < best_val_loss:\n",
    "        best_val_loss = eval_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "         \n",
    "    print(f\"{I}\\t{train_loss:.5f}\\t{train_acc:.2f}\\t{train_f1:.2f}\\t{train_prec:.2f}\\t{train_rec:.2f}\\t{eval_loss:.5f}\\t{eval_acc:.2f}\\t{eval_f1:.2f}\\t{eval_prec:.2f}\\t{eval_rec:.2f}\\t{metrics['best']:.2f}\")\n",
    "\n",
    "    if (patience_counter >= patience):\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "with open(log_path + \"/metrics.json\", \"w\") as fp:\n",
    "    json.dump(metrics, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fa26b6-9332-426f-a327-da656aa29969",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b36656a-4757-440a-87e1-48aaf5cafb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\t\".join([\"Test Acc\", \"Test F1\", \"Test Precision\", \"Test Recall\"]))\n",
    "loss, acc, f1, prec, rec = trainer.test()\n",
    "print(f\"{acc:.2f}\\t{f1:.2f}\\t{prec:.2f}\\t{rec:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0199d0af-fd41-4123-b9a3-8e7e503c24c4",
   "metadata": {},
   "source": [
    "## Save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f209e0e-3ab4-4e30-8c2c-8e94f63a0b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(model, word2idx, file_path):\n",
    "    embeddings = model.get_embeddings().detach().cpu().numpy().astype(np.float32)\n",
    "    vocab_size = len(word2idx)\n",
    "    emb_dim = embeddings.shape[1]\n",
    "\n",
    "    with open(file_path, 'wb') as f:\n",
    "        header = f\"{vocab_size} {emb_dim}\\n\"\n",
    "        f.write(header.encode('utf-8'))\n",
    "\n",
    "        for word, idx in word2idx.items():\n",
    "            word_part = word.encode('utf-8') + b' '\n",
    "            f.write(word_part)\n",
    "            f.write(embeddings[idx].tobytes())\n",
    "\n",
    "    \n",
    "def save_mat(file_path, tens):\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(tens.detach().cpu().numpy().T.tobytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a5436d-5d3e-4956-8266-2b2305908719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv\n",
    "# [num_filters, kernel_size * emb_dim]\n",
    "# [num_filters]\n",
    "convs, bias = model.get_filters()\n",
    "# domains\n",
    "# [num_domains, num_filters]\n",
    "at_dom = model.get_attention_domains()\n",
    "# embeddings\n",
    "# [vocab_size, emb_dim]\n",
    "emb = model.get_embeddings()\n",
    "# bn_conv\n",
    "# [num_filters]\n",
    "alpha = model.bn_conv.weight\n",
    "# [num_filters]\n",
    "beta = model.bn_conv.bias\n",
    "# [num_filters]\n",
    "mean = model.bn_conv.running_mean\n",
    "# [num_filters]\n",
    "var = model.bn_conv.running_var\n",
    "# fc layer\n",
    "lst = model.get_fc_weights()\n",
    "w_lst = []\n",
    "b_lst = []\n",
    "for elem in lst:\n",
    "    w,b = elem\n",
    "    w_lst.append(w)\n",
    "    b_lst.append(b)\n",
    "# [num_domains * num_classes, num_filters]    \n",
    "fc_mat = torch.cat(w_lst, dim=0)\n",
    "# [num_domains * num_classes] \n",
    "fc_bias = torch.cat(b_lst, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73bfd6f-92a3-4231-a6cf-98ff3b4587a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embeddings(model, train_dataset.token_to_idx, weights_path + \"/embeddings.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918ebc36-dc38-4387-8778-6780587e7cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_mat(f\"{weights_path}/conv_matrix.bin\", convs)\n",
    "save_mat(f\"{weights_path}/conv_bias.bin\", bias)\n",
    "save_mat(f\"{weights_path}/attention_domains.bin\", at_dom)\n",
    "save_mat(f\"{weights_path}/bn_alpha.bin\", alpha)\n",
    "save_mat(f\"{weights_path}/bn_beta.bin\", beta)\n",
    "save_mat(f\"{weights_path}/bn_mean.bin\", mean)\n",
    "save_mat(f\"{weights_path}/bn_var.bin\", var)\n",
    "save_mat(f\"{weights_pathh}/fc_matrices.bin\", fc_mat)\n",
    "save_mat(f\"{weights_path}/fc_biases.bin\", fc_bias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
